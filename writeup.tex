\documentclass[12pt]{article}
\usepackage[margin=1 in]{geometry}
\begin{document}

\title{CS51 Final Project Writeup}
\author{Joey Bejjani}
\date{May 3, 2023}

\maketitle

\qquad For my MiniML extension, I implemented an evaluator using the lexically-scoped environment model (number 2 in the extension ideas list in the readme). I did this by first copying the {\tt eval\_d} function definition and pasting into the {\tt eval\_l} function definition, since they share a lot of the same semantics rules. I then modified the copied code to manifest lexical scoping. There were three modifications, which are roughly as follows: first, the evaluation of a function returns a closure with the function and the current environment instead of just the function as in {\tt eval\_d}; second, function application evaluates the body of the function in the environment from its updated closure; third, evaluation of a {\tt let rec} expression involves binding the variable to {\tt Unassigned} and later mutating this mapping. The resulting {\tt eval\_l} function passed both my generic {\tt eval\_tests} (which includes expressions whose evaluations dynamic and lexical semantics models should agree on) and my {\tt eval\_lexical\_tests} (which includes expressions whose evaluations differ depending on whether lexical or dynamic semantics rules are used); however, {\tt eval\_d} and {\tt eval\_l} contained a lot of redundant code as a result of the copy and pasting. I factored out the shared code using the modular programming paradigm. I created an {\tt ENV\_SCOPE} type that would package together the three differences between the semantics rules of the dynamic and lexical environment models. I then defined two modules of this type, {\tt EnvDynamic} and {\tt EnvLexical}, which implement the evaluation of function expressions, application expressions, and {\tt let rec} expressions according to the appropriate semantics rules. I then defined a functor {\tt MakeEnvEvaluator} satisfying an {\tt ENV\_EVALUATOR} type and taking an {\tt ENV\_SCOPE} as an argument. The functor returns a module (an {\tt ENV\_EVALUATOR}) consisting of the recursive {\tt eval} function. This function evaluates the argument expression inside the argument environment, calling the {\tt f}, {\tt app}, and {\tt letrec} functions defined in the argument {\tt ENV\_SCOPE} module in order to evaluate function, application, and {\tt let rec} expressions, respectively. The {\tt MakeEnvEvaluator} functor thus returns a module whose {\tt eval} function evaluates expressions according to the scope manifested in the definition of its argument {\tt ENV\_SCOPE} module. I then applied {\tt MakeEnvEvaluator} to {\tt EnvDynamic} to get the module whose {\tt eval} function is equivalent to {\tt eval\_d}, and then again to {\tt EnvLexical} to get the module whose {\tt eval} function is {\tt eval\_l}. The two evaluators obtained using this abstracted approach passed all the same tests that they passed in the redundant version. Importantly, {\tt eval\_s} and {\tt eval\_l} agree on the evaluations of the expressions in the {\tt eval\_lexical\_tests}, verifying that they both manifest lexical scoping.
\par I also added the {\tt string} atomic type along with the {\tt Concat} (string concatenation) binary operator as a second extension. I did this by extending the definition of the {\tt expr} type to include {\tt String of string} and extending the {\tt binop} type to include {\tt Concat}. I updated {\tt expr.mli} to reflect these type definition changes. In {\tt free\_vars} and {\tt subst}, I added a match case to handle the new {\tt String} expression. I extended {\tt exp\_to\_concrete\_string}, {\tt exp\_to\_abstract\_string}, and my {\tt string\_of\_binop} function to handle {\tt Concat} and {\tt String}. In my {\tt binop\_eval} function, I raised {\tt EvalError}s when anything other than two strings are being concatenated, and also when algebraic operators are applied to strings. To extend the language to include strings and concatenation, I extended the parser with the following steps. In {\tt miniml\_lex.mll}, I added {\tt ("\string^", CONCAT)} as an entry in the {\tt sym\_table} hashtable. I added a new regular expression {\tt string} that matches user input that starts with a `` character and ends with a " character. A matched {\tt string} is then parsed by stripping the quotation marks by using the {\tt String.sub} function and returning it as a {\tt STRING} token. I defined this new token using {\tt \%token <string> STRING} in {\tt miniml\_parse.mly} along with {\tt \%token CONCAT}, which I also specified as left associative using {\tt \%left CONCAT}. Then, in {\tt expnoapp}, I specified that inputs of the form {\tt exp CONCAT exp} parse to {\tt Binop(Concat, \$1, \$3)}, where \$1 and \$3 specify the two {\tt exp} arguments (in the first and third positions). Finally, {\tt String} tokens parse to {\tt String \$1}, where \$1 is the actual string returned after stripping the quotation marks inputted by the user as described earlier. I wrote and successfully ran more tests to verify that strings and concatenation work as intended.

\end{document}
